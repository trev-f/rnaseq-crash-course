[
  {
    "objectID": "pipeline/trim_reads.html",
    "href": "pipeline/trim_reads.html",
    "title": "Trim Reads",
    "section": "",
    "text": "alpha stage warning\n\n\n\nThis website is in its alpha stage. All pages are under active development and are subject to change at any time without warning."
  },
  {
    "objectID": "pipeline/trim_reads.html#learning-objectives",
    "href": "pipeline/trim_reads.html#learning-objectives",
    "title": "Trim Reads",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nTrim raw reads with fastp\nGenerate trimmed reads QC reports with FastQC\nProduce a composite trimmed reads QC report with MultiQC"
  },
  {
    "objectID": "pipeline/trim_reads.html#purpose",
    "href": "pipeline/trim_reads.html#purpose",
    "title": "Trim Reads",
    "section": "Purpose",
    "text": "Purpose\nEven successful and high quality sequencing runs produce some problematic reads or portions of reads. The most common issues we run into include low quality base calls and the presence of adapters at the ends of reads. These problematic reads can cause issues in data quality in downstream steps, so we use read trimming to remove them.\nAfter read trimming, we run FastQC on the trimmed reads to make sure the trimming mitigated any issues observed at the raw reads QC step."
  },
  {
    "objectID": "pipeline/trim_reads.html#fastp-usage",
    "href": "pipeline/trim_reads.html#fastp-usage",
    "title": "Trim Reads",
    "section": "fastp usage",
    "text": "fastp usage\n\nSoftware availability\nfastp docs\nfastp is available as Apptainer containers through BioContainers:\n\n\nTerminal\n\nfunction fastp {\n  local image_uri=https://depot.galaxyproject.org/singularity/fastp:0.23.4--hadf994f_3\n  apptainer exec \"${image_uri}\" fastp \"${@}\"\n}\n\nThis method makes fastp available through the fastp command.\n\n\nCommand usage\nBasic usage:\n\n\nTerminal\n\nfastp \\\n  --in1 sample1.fastq.gz \\\n  --out1 sample1_trimmed.fastq.gz"
  },
  {
    "objectID": "pipeline/trim_reads.html#fastp-sbatch-script",
    "href": "pipeline/trim_reads.html#fastp-sbatch-script",
    "title": "Trim Reads",
    "section": "fastp sbatch script",
    "text": "fastp sbatch script\nRun fastp in parallel using --ntasks and srun.\n\n\ntrim_reads_fastp.sh\n\n#!/bin/bash\n\n#SBATCH --job-name=trim_reads_fastp\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4GB\n#SBATCH --time=00-00:30:00\n&lt;... Remaining constant SBATCH flags ...&gt;\n\n\n#######################################\n# Constants\n#######################################\n# Array of raw fastq files\nreadonly fastqs=(data/reads/raw/*.fastq.gz)\n# Output directory paths\nreadonly out_data_dir=\"data/reads/trim\"\nreadonly out_reports_dir=\"reports/fastp\"\n\n\n#######################################\n# Load software\n#######################################\n# echo to stderr\nfunction errcho {\n    &gt;&2 echo \"${@}\"\n}\nfunction fastp {\n  local image_uri=https://depot.galaxyproject.org/singularity/fastp:0.23.4--hadf994f_3\n  apptainer exec \"${image_uri}\" fastp \"${@}\"\n}\n\n\n#######################################\n# Main program.\n#######################################\nfor fastq in \"${fastqs[@]}\"; do\n  errcho \"Trim reads: ${fastq}\"\n  sample_name=$(basename $fastq .fastq.gz)\n  srun --ntasks 1 --cpus-per-task 4 \\\n    fastp \\\n    --thread \"${SLURM_CPUS_PER_TASK}\" \\\n    --in1 \"${fastq}\" \\\n    --out1 \"${out_data_dir}/${sample_name}_trimmed.fastq.gz\" \\\n    --json \"${out_reports_dir}/${sample_name}_fastp.json\" \\\n    --html \"${out_reports_dir}/${sample_name}_fastp.html\" \\\n    &\ndone\nwait"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RNA-seq Crash Course",
    "section": "",
    "text": "alpha stage warning\n\n\n\nThis website is in its alpha stage. All pages are under active development and are subject to change at any time without warning.\n\n\n\n\n\n\n\nRNA-seq Crash Course\nThis is the website for the RNA-seq Crash Course by the University of Tennessee Institute of Agriculture Genomics Center."
  },
  {
    "objectID": "computing/isaac-ng.html",
    "href": "computing/isaac-ng.html",
    "title": "ISAAC-NG",
    "section": "",
    "text": "alpha stage warning\n\n\n\nThis website is in its alpha stage. All pages are under active development and are subject to change at any time without warning."
  },
  {
    "objectID": "computing/isaac-ng.html#access-and-login",
    "href": "computing/isaac-ng.html#access-and-login",
    "title": "ISAAC-NG",
    "section": "Access and login",
    "text": "Access and login\nEasiest: Browser login through Open OnDemand\nhttps://login.isaac.utk.edu/"
  },
  {
    "objectID": "computing/isaac-ng.html#running-jobs",
    "href": "computing/isaac-ng.html#running-jobs",
    "title": "ISAAC-NG",
    "section": "Running jobs",
    "text": "Running jobs\nTasks that utilize a lot of computing resources should never be run on the login nodes. Any tasks beyond basic ones such as file editing and management, code compliation, and job submission must be run on compute nodes.\n\nInteractive sessions\nOne way to run jobs on compute nodes is launch an interactive session. Interactive sessions allow us to access and interact with compute nodes through the terminal in essentially the same way we interact with the login node.\nThe main benefit of interactive sessions is just that – we can utilize the resources that are available on ISAAC-NG by directly interacting with them. This makes interactive sessions ideal for short sessions such as testing commands and developing scripts because we can directly run commands on a compute node without the need to wait for Slurm to allocate resources, configure jobs, and setup environments every time we tweak a program. However, interactive sessions are killed when we disconnect from the terminal, so for longer running tasks batch jobs are recommended.\nThe command srun --pty /bin/bash -i is my preferred way of launching an interactive session, and this is how we will launch jobs in this crash course1. However, ISAAC-NG requires us to explicitly provide an account and a valid partition–QOS combination. Because interactive sessions are designed to be short-lived, we typically want to run them on the short partition and QOS so that they are launched more quickly and so we’re not taking up resources from other partitions.\nHere is a typical full command that we can use to launch an interactive session:\n\n\nTerminal\n\nsrun \\\n  --account ACF-UTK0011 \\\n  --partition short \\\n  --qos short \\\n  --cpus-per-task 4 \\\n  --pty \\\n  /bin/bash -i\n\n\nLaunch interactive sessions with environment variables\nThe whole point of running interactive jobs is to quickly and easily get a job on a compute node, but entering srun command above is anything but quick and easy. Luckily with Slurm we can make things easier on ourselves by using environment variables. Slurm commands such as srun and salloc can use specific environment variables to configure resource requests for jobs so that we don’t have to explicitly supply them on the command line. We will also make use of setting these environment variables inside of a config file so that we don’t have to type them in manually every time we want to set them.\nThe first step is to create the config file of Slurm environment variables for interactive session2:\n\n\n~/.config/slurm-env/interactive-slurm.env\n\n# srun uses prefix 'SLURM_'\n# https://slurm.schedmd.com/srun.html#SECTION_INPUT-ENVIRONMENT-VARIABLES\nexport SLURM_ACCOUNT=\"ACF-UTK0011\"\nexport SLURM_PARTITION=\"short\"\nexport SLURM_QOS=\"short\"\nexport SLURM_TRES_PER_TASK=\"cpu:4\"\nexport SLURM_TIMELIMIT=\"03:00:00\"\n\n# salloc uses prefix 'SALLOC_'\n# https://slurm.schedmd.com/salloc.html#lbAI\nexport SALLOC_ACCOUNT=\"ACF-UTK0011\"\nexport SALLOC_PARTITION=\"short\"\nexport SALLOC_QOS=\"short\"\nexport SALLOC_TRES_PER_TASK=\"cpu:4\"\nexport SALLOC_TIMELIMIT=\"03:00:00\"\n\nNow, whenever you want to launch an interactive session, you just have to load the environment variables and execute the srun command:\n\n\nTerminal\n\nsource ~/.config/slurm-env/interactive-slurm.env\nsrun --pty /bin/bash -i\n\n\n\nLaunch interactive session with single command\nWe can take this a step further to allow us to launch an interactive session by simply running the command launch-int.\nAdd the following to your ~/.bash_aliases file3:\n\n\n~/.bash_aliases\n\n# Launch an interactive session on a compute node\nfunction launch-int {\n    source ~/.config/slurm-env/interactive-slurm.env\n1    srun \"${@}\" --pty /bin/bash -i\n}\n\n\n1\n\nThe \"${@}\" let’s us pass any argument to srun so that we can add additional configure or override configuration that is in our environment variables. For Slurm commands like srun, command line arguments override values stored in environment variables.\n\n\nLaunching an interactive compute session is now as quick and simple as running a single command:\n\n\nTerminal\n\nlaunch-int"
  },
  {
    "objectID": "computing/isaac-ng.html#footnotes",
    "href": "computing/isaac-ng.html#footnotes",
    "title": "ISAAC-NG",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsalloc is the officially recommended method for launching interactive sessions. I find salloc less user friendly as it requires us to SSH into the reserved compute node to access our job which moves us into our home directory. srun --pty /bin/bash -i drops us straight into our interactive session inside our current working directory without having to run any other commands.↩︎\nIf you don’t have a slurm-env directory you will have to create it with mkdir -p ~/.config/slurm-env. There isn’t anything special about this directory or file name, but it’s good practice to put config files like this inside a subdirectory of ~/.config.↩︎\nIf ~/.bash_aliases doesn’t exist, you will have to create the file.\nAlso verify that your .bash_aliases file is sourced in your ~/.bashrc file, or else this function won’t be automatically made available in new terminal sessions.\n\n\n~/.bashrc\n\n# User specific aliases and functions\nif [ -f ~/.bash_aliases ]; then\n    . ~/.bash_aliases\nfi\n\nNote that ~/.bashrc and ~/.bash_aliases are special filenames for bash. These should be used by convention.↩︎"
  },
  {
    "objectID": "computing/hpc.html",
    "href": "computing/hpc.html",
    "title": "High Performance Computing",
    "section": "",
    "text": "alpha stage warning\n\n\n\nThis website is in its alpha stage. All pages are under active development and are subject to change at any time without warning."
  },
  {
    "objectID": "computing/hpc.html#basic-computing-terminology",
    "href": "computing/hpc.html#basic-computing-terminology",
    "title": "High Performance Computing",
    "section": "Basic computing terminology",
    "text": "Basic computing terminology\n\nResources and computer components\n\nJob\n\nSome unit of work or collection of tasks that needs to be performed by a computing system. In typical usage this is not well defined, and the scope of what a job is can vary wildly. I typically use job to refer to some logically connected group of tasks that I want the computer to perform.\n\nTask\n\nA small unit of work, usually a specific operation performed by the computer. Often a part of a larger job.\n\nCPU (central processing unit)\n\nThe core computational unit of a PC or node. This is the hardware component that performs computations. AKA: Processor.\n\nMemory / RAM\n\nSite of temporary information storage for holding data while computations are performed on the data. Important note: this is distinct from data storage.\n\nData storage / disk space\n\nPermanent or long term storage of data.\n\n\n\n\nAn imperfect analogy\nThroughout this page, we will use an analogy to help understand computing. The goal here is to develop a mental model of computing and the interplay of computing components and resources that enables users to more effectively and efficiently utilize the resources they have access to.\nThink about computing as being like a bakery. At this bakery, the overall goal is to prepare all of the menu items and operate the store front.\nThere are many jobs at this bakery: baking croissants, preparing pastry creams and jams, stocking the pastry cases, and washing dishes among many others. Each job is composed of specific tasks: kneading bread dough and placing doughnuts in a pastry case are just a couple of examples. These similarities follow pretty naturally from how we use the terms in computing.\nThese jobs and tasks obviously aren’t going to perform themselves. Someone must do these tasks, and they have to have physical resources to perform their tasks. We can think of the bakers as being like CPUs. The bakers are who physically perform the tasks of kneading and placing according to recipes and standard operating procedures, much like the CPU performs computations based on sets of instructions called programs. In this analogy the raw ingredients, goods in progress like dough, and kitchen equipment such as mixers represent memory. The ingredients and equipment are used temporarily by the bakers, often to transform one thing into another such as shaping a ball of dough into a loaf or using a dish brush to wash a plate. This is much like the role memory plays in computing, where it temporarily holds data while the CPU performs computations on the data. Finally, there are many parts of a bakery that act like data storage. This includes the pastry case that holds the finished baked goods in the store front, the shelves in the kitchen that hold equipment, and even the recipe book."
  },
  {
    "objectID": "computing/hpc.html#what-is-an-hpc",
    "href": "computing/hpc.html#what-is-an-hpc",
    "title": "High Performance Computing",
    "section": "What is an HPC?",
    "text": "What is an HPC?\n\n\n\nHPC schematic\n\n\n\nA collection (cluster) of many computers (nodes) connected together\nEach node is a collection of CPUs and RAM with a small amount of associated disk space\nLarge amount of shared data storage disks connected to nodes provides a huge amount of storage space\nProvides access for a large number of users\nTypically a login node that functions as the main way users interact with the HPC\n\nKeeps the compute nodes free to handle heavy computing\n\nTypically a scheduling system to manage requirements for large number of users who have varying needs\n\nAnalogy: If a PC is like a small mom and pop bake shop, then an HPC is an industrial scale kitchen.\n\nHave all of the same components but operate on different scales\nLarger scale means more complexity which means different logistics\n\nCan’t walk up to a store front at a Hostess factory and place an order for a fresh Twinkie"
  },
  {
    "objectID": "computing/hpc.html#why-use-an-hpc",
    "href": "computing/hpc.html#why-use-an-hpc",
    "title": "High Performance Computing",
    "section": "Why use an HPC?",
    "text": "Why use an HPC?\nTwo words: More power\n\nEach node has more resources than a typical PC\n\nMore CPUs means you can run more tasks at one time\nMore RAM means you can run tasks that a PC can’t handle\nMore disk space means you can work with more data than a PC can handle\n\nPresence of many compute nodes multiplies the benefits\n\nConsider the ISAAC-NG cluster at UTK: More than 200 nodes and over 12,000 cores!"
  },
  {
    "objectID": "computing/hpc.html#other-resources",
    "href": "computing/hpc.html#other-resources",
    "title": "High Performance Computing",
    "section": "Other resources",
    "text": "Other resources\nMuch of this was motivated by the HBC Training HPC intro. Check out their page especially for thoughts on multithreading and parallelization."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "alpha stage warning\n\n\n\nThis website is in its alpha stage. All pages are under active development and are subject to change at any time without warning.\n\n\n\n\n\n\n\nAbout"
  },
  {
    "objectID": "pipeline/raw_fastqc.html",
    "href": "pipeline/raw_fastqc.html",
    "title": "Raw Reads FastQC",
    "section": "",
    "text": "alpha stage warning\n\n\n\nThis website is in its alpha stage. All pages are under active development and are subject to change at any time without warning."
  },
  {
    "objectID": "pipeline/raw_fastqc.html#learning-objectives",
    "href": "pipeline/raw_fastqc.html#learning-objectives",
    "title": "Raw Reads FastQC",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nGenerate raw reads QC reports with FastQC\nProduce a composite raw reads QC report with MultiQC"
  },
  {
    "objectID": "pipeline/raw_fastqc.html#purpose",
    "href": "pipeline/raw_fastqc.html#purpose",
    "title": "Raw Reads FastQC",
    "section": "Purpose",
    "text": "Purpose\nRaw reads QC should be performed to ensure that the quality of reads that came off of the sequencing platform is acceptable. FastQC is a straightforward command line tool for generating QC reports for reads in fastq format.\nA few of the metrics we’re most interested in include:\n\nTotal number of reads and bases.\nAverage quality along reads.\nOverrepresented sequences.\nAdapter contamination."
  },
  {
    "objectID": "pipeline/raw_fastqc.html#fastqc-usage",
    "href": "pipeline/raw_fastqc.html#fastqc-usage",
    "title": "Raw Reads FastQC",
    "section": "FastQC usage",
    "text": "FastQC usage\n\nSoftware availability\nFastQC is available as a module that can be loaded on ISAAC:\n\n\nTerminal\n\nmodule load fastqc/0.11.9\n\nAlternatively, more recent versions are available as Apptainer containers through BioContainers:\n\n\nTerminal\n\nfunction fastqc {\n  local image_uri=https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0\n  apptainer exec \"${image_uri}\" fastqc \"${@}\"\n}\n\n\n\nCommand usage\nBoth of the methods above make FastQC available through the fastqc command.\nBasic usage:\n\n\nTerminal\n\n1fastqc sample1.fastq.gz sample2.fastq.gz\n\n\n1\n\nAn arbitrary number of sequence files can be specified. Multiple sequence files must be separated by spaces. In practice we typically run FastQC on (gzipped) fastq files, but other sequence file formats can be used (run fastqc --help for details).\n\n\nUseful options:\n\n\nTerminal\n\nfastqc \\\n1  --outdir reports/fastqc \\\n2  --threads 2 \\\n  sample1.fastq.gz sample2.fastq.gz\n\n\n1\n\nThe directory to write output files into. If this directory does not exist, the command will fail with an error.\n\n2\n\nThe number of threads to use. This is the number of files that FastQC will process simultaneously."
  },
  {
    "objectID": "pipeline/raw_fastqc.html#multiqc-usage",
    "href": "pipeline/raw_fastqc.html#multiqc-usage",
    "title": "Raw Reads FastQC",
    "section": "MultiQC usage",
    "text": "MultiQC usage\n\nSoftware availability\n\n\n\n\n\n\nMultiQC version\n\n\n\nMultiQC had a significant update around v1.19 that made its reports significantly better from a user standpoint, especially when working with a large number of samples. I strongly recommend using the most recent MultiQC version (v1.21 at the time of writing this).\n\n\nMultiQC is available as Apptainer containers through BioContainers:\n\n\nTerminal\n\nfunction multiqc {\n  local image_uri=https://depot.galaxyproject.org/singularity/multiqc:1.21--pyhdfd78af_0\n  apptainer exec \"${image_uri}\" multiqc \"${@}\"\n}\n\n\n\nCommand usage\nThe method above makes MultiQC available through the multiqc command.\nBasic usage:\n\n\nTerminal\n\n1multiqc .\n\n\n1\n\nThis will traverse through all files and directories in the current working directory (.) and add any files that MultiQC recognizes to the report that it generates.\n\n\nUseful options:\n\n\nTerminal\n\nmultiqc \\\n1  --filename raw_reads_qc \\\n2  --outdir reports \\\n3  --title \"Raw reads QC\" \\\n  .\n\n\n1\n\nFilename to use for the MultiQC report and data files.\n\n2\n\nThe directory to write output files into.\n\n3\n\nA title for the report."
  }
]